## Gradient Accumulation
### Pytorch手动梯度清零模式的好处
传统的训练函数
```python
model = ...
criterion = ...
optimizer = ...
train_loader = ...
for i,(images,target) in enumerate(train_loader):
    # 1. input output
    images = images.cuda(non_blocking=True)
    target = target.cuda(non_blocking=True)
    outputs = model(images)
    loss = criterion(outputs,target)

    # 2. backward
    optimizer.zero_grad()   # reset gradient
    loss.backward()
    optimizer.step()
```
- 计算损失函数
- `optimizer.zero_grad()`, 清零过往梯度
- `loss.backward()`, 反向传播，计算当前梯度
- `optimizer.step()`, 根据梯度更新网络参数

使用梯度累加计算梯度两种方式: 区别于 `loss function` 函数中 `reduction='mean'` 和 `reduction='sum'` 的不同

- `reduction='mean'`的情况下
```python
model = ...
criterion = ...
optimizer = ...
train_loader = ...
gradient_accumulation_steps = 4
for i,(images,target) in enumerate(train_loader):
    # 1. input output
    images = images.cuda(non_blocking=True)
    target = target.cuda(non_blocking=True)
    outputs = model(images)
    loss = criterion(outputs,target)

    # 2.1 loss regularization
    loss = loss / gradient_accumulation_steps   
    # 2.2 back propagation
    loss.backward()
    # 3. update parameters of net
    if((i+1) % gradient_accumulation_steps)==0:
        # optimizer the net
        optimizer.step()        # update parameters of net
        optimizer.zero_grad()   # reset gradient
```

- `reduction='sum'`的情况下
```python
model = ...
criterion = ...
optimizer = ...
train_loader = ...
gradient_accumulation_steps = 4
for i,(images,target) in enumerate(train_loader):
    # 1. input output
    images = images.cuda(non_blocking=True)
    target = target.cuda(non_blocking=True)
    outputs = model(images)
    loss = criterion(outputs,target)

    # 2.1 不需要进行 regularization 操作
    # loss = loss / gradient_accumulation_steps
    # 2.2 back propagation
    loss.backward()
    # 3. update parameters of net
    if((i+1) % gradient_accumulation_steps)==0:
        # optimizer the net
        optimizer.step()        # update parameters of net
        optimizer.zero_grad()   # reset gradient
```
- 计算损失函数
- `loss.backward()`, 反向传播，计算当前梯度
- 循环 gradient_steps 次 __计算损失函数-反向传播计算梯度__ 操作，将梯度累积到之前的梯度上
- `optimizer.step()`, 根据梯度更新网络参数
- `optimizer.zero_grad()`, 清空过往梯度，为下一次循环梯度累积做准备

本人比较喜欢的一种写法:
```python
model = ...
criterion = ...
optimizer = ...
train_loader = ...
gradient_accumulation_steps = 4
# 将大的 batch_size 分成 sub_batch_size
batch_size = ...
sub_batch_size = int( batch_size / gradient_accumulation_steps )

for i,(images,target) in enumerate(train_loader):
    optimizer.zero_grad()
    for accu_step in range(gradient_accumulation_steps):
        sub_images = images[accu_step * sub_batch_size:
                            (accu_step + 1) * sub_batch_size].cuda(non_blocking=True)
        sub_labels = target[accu_step * sub_batch_size:
                            (accu_step + 1) * sub_batch_size].cuda(non_blocking=True)
        outputs = model(sub_images)
        loss = criterion(outputs, sub_labels)
        if loss_reduction == 'mean': # 在mean的情况下需要进行regularization操作
            loss = loss / gradient_accumulation_steps
        loss.backward()

    optimizer.step()
```


### Details
#### 1. loss regularization
在`reduction='mean'`下loss对gradient_accumulation_steps进行再平均 
```python
loss = criterion(outputs,target)
loss = loss / gradient_accumulation_steps   
loss.backward()
```
- 因为在`reduction='mean'`的情况下, 每次求出的loss是一个batch内预测和标签误差的平均值，使用梯度累计的时候求出几个batch_size的平均值，进行一次再平均，等效于大batch_size的近似平均

在`reduction='sum'`下不需要进行regularization操作，通过计算可知，不进行regularization操作，和大batch_size的loss等价

#### 2. learning rate
使用的损失函数中, `reduction`参数设置不同，学习率也应进行相应调整
- 在`reduction='sum'`的情况下, 学习率要相应的调小，因为loss比较大
- 在`reduction='mean'`的情况下，学习率相应调大，因为loss比较小

### Implemented Work
- [Pytorch为什么需要手动梯度清零](https://www.zhihu.com/question/303070254/answer/573037166)
