## DDP Training with apex
### install apex
第一种方法 (followed NVIDIA apex [repo](https://github.com/NVIDIA/apex)): 
```bash
$ git clone https://github.com/NVIDIA/apex
$ cd apex
$ pip install -v --no-cache-dir --global-option="--pyprof" --global-option="--cpp_ext" --global-option="--cuda_ext" ./
```
第二种方法 (followed from this [issue](https://github.com/NVIDIA/apex/issues/86#issuecomment-455620478))
```bash
$ git clone https://github.com/NVIDIA/apex
$ cd apex
$ python setup.py --install --cuda_ext --cpp_ext
```
第三种方法 (followed from this [pull](https://github.com/NVIDIA/apex/pull/323#issuecomment-567947218))
```bash
$ git clone https://github.com/NVIDIA/apex
$ cd apex
$ python setup.py install
```

Addition:
  - 如果安装失败，记得确认pytorch版本和cuda版本是否支持, pytorch >= 1.6, 确认版本一致后重新安装
```bash
$ cd apex
$ rm -rf build/
$ pip uninstall apex
$ python setup.py --install --cuda_ext --cpp_ext
```

### Using mixed-precision training with apex
#### 1. import relative package
```python
try:
    import apex
    from apex.parallel import DistributedDataParallel as DDP
    from apex.fp16_utils import *
    from apex import amp, optimizers
    from apex.multi_tensor_apply import multi_tensor_applier
except ImportError:
    raise ImportError("Please install apex from https://www.github.com/nvidia/apex to run this example.")
```

#### 2. update the code
大部分代码与`torch.distributed`保持一致，使用的时候只需要将`torch.nn.parallel.DistributedDataParallel`替换为`apex.parallel.DistributedDataParallel`, apex可以帮助我们自动管理device，代码层面上可以少写一些
```python
model = ...
optimizer = ...
model, optimizer = amp.initialize(model, optimizer) # 加上这句
# model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank])
model = DDP(model) # 不需要添加 device_ids=[local_rank]
```

修改Loss, 加上scaled loss修饰符
```python
optimizer.zero_grad()
# loss.backward()
with amp.scale_loss(loss, optimizer) as scaled_loss:
    scaled_loss.backward()
optimizer.step()
```
其余和正常训练保持一致即可

### Details
#### 1. apex.parallel.convert_syncbn_model
在使用时如果需要用`SyncBatchNorm`操作, 需要用apex提供的API
```python
# model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(local_rank)
model = apex.parallel.convert_syncbn_model(model).to(local_rank)
```
